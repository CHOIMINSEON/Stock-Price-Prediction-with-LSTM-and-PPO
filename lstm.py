import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error # ì¶”ê°€ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬
import matplotlib.pyplot as plt # ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬
import joblib
import os

# ==========================================
# [1] ì„¤ì • ì˜ì—­
# ==========================================
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
DATA_DIR = os.path.join(BASE_DIR, "data")
MODEL_DIR = os.path.join(BASE_DIR, "models")
RESULT_DIR = os.path.join(BASE_DIR, "results") # ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥ í´ë”

for path in [MODEL_DIR, RESULT_DIR]:
    if not os.path.exists(path):
        os.makedirs(path)

# ì‚¬ìš©í•  ì¢…ëª© ë¦¬ìŠ¤íŠ¸
TICKERS = ['AAPL', 'AMD', 'AMZN', 'GOOGL', 'META', 'NVDA', 'PLTR', 'TSLA']

# ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°
SEQ_LENGTH = 60       
HIDDEN_SIZE = 64      
NUM_LAYERS = 2        
EPOCHS = 50           
BATCH_SIZE = 32       
LEARNING_RATE = 0.001 

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸš€ í•™ìŠµ ì¥ì¹˜: {device}")

# í•™ìŠµì— ì‚¬ìš©í•  ë³€ìˆ˜ë“¤
FEATURES = [
    'Open', 'High', 'Low', 'Close', 'Volume', 
    'RSI', 'MACD', 'MACD_Signal', 'MA20', 
    'ATR', 'VWAP', 'VIX', 'TNX', 'DXY', 'QQQ', 'XLK',
    'DayOfWeek', 'Hour'
]

# ==========================================
# [2] LSTM ëª¨ë¸ ì •ì˜
# ==========================================
class StockLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(StockLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :]) 
        return out

# ==========================================
# [3] ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
# ==========================================
def create_sequences(data, seq_length):
    xs, ys = [], []
    for i in range(len(data) - seq_length):
        x = data[i:(i + seq_length)]
        y = data[i + seq_length][3] # Index 3 is 'Close' (Target)
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

# ==========================================
# [4] í•™ìŠµ ë° í‰ê°€ ë©”ì¸ ë¡œì§
# ==========================================
def train_lstm(ticker):
    print(f"\n[{ticker}] ë°ì´í„° ë¡œë”© ì¤‘...")
    file_path = os.path.join(DATA_DIR, f"{ticker}_hourly_alp_yf_dataset_v2.csv")
    
    if not os.path.exists(file_path):
        print(f"íŒŒì¼ ì—†ìŒ: {file_path}")
        return

    df = pd.read_csv(file_path)
    
    # ì‹œê°„ìˆœ ì •ë ¬
    if 'timestamp' in df.columns:
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df = df.sort_values('timestamp')
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
    available_features = [f for f in FEATURES if f in df.columns]
    data = df[available_features].values
    
    # Scaling
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler() # ì¢…ê°€(Close)ë§Œ ë”°ë¡œ ìŠ¤ì¼€ì¼ë§ (ì—­ë³€í™˜ ìœ„í•´)

    scaled_data = scaler_X.fit_transform(data)
    
    # Target(Close) Scaling ë³„ë„ ì €ì¥
    close_idx = available_features.index('Close')
    scaler_y.fit(df[['Close']]) 

    X, y = create_sequences(scaled_data, SEQ_LENGTH)

    # Train/Val Split (8:2)
    split_idx = int(len(X) * 0.8)
    X_train, y_train = X[:split_idx], y[:split_idx]
    X_val, y_val = X[split_idx:], y[split_idx:]

    # Tensor ë³€í™˜
    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)
    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device)
    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)
    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(device)

    # DataLoader
    train_data = TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)

    # ëª¨ë¸ ì´ˆê¸°í™”
    model = StockLSTM(input_size=len(available_features), hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, output_size=1).to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # í•™ìŠµ ë£¨í”„
    best_loss = float('inf')
    patience = 0
    
    print(f"[{ticker}] í•™ìŠµ ì‹œì‘...")
    for epoch in range(EPOCHS):
        model.train()
        train_loss = 0
        for batch_X, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val_tensor)
            val_loss = criterion(val_outputs, y_val_tensor)
        
        avg_train_loss = train_loss / len(train_loader)
        
        if (epoch+1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{EPOCHS}] Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss.item():.6f}")

        # Early Stopping check
        if val_loss.item() < best_loss:
            best_loss = val_loss.item()
            patience = 0
            torch.save(model.state_dict(), f"{MODEL_DIR}/{ticker}_lstm.pth")
        else:
            patience += 1
            if patience > 5:
                print("Early Stopping triggered.")
                break
            
    # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    joblib.dump(scaler_X, f"{MODEL_DIR}/{ticker}_scaler_X.pkl")
    joblib.dump(scaler_y, f"{MODEL_DIR}/{ticker}_scaler_y.pkl") # y ìŠ¤ì¼€ì¼ëŸ¬ë„ ì €ì¥

    # ==========================================
    # [5] (New) ìƒì„¸ í‰ê°€ ë° ì‹œê°í™”
    # ==========================================
    evaluate_model(model, X_val_tensor, y_val_tensor, scaler_y, ticker)

def evaluate_model(model, X_val, y_val, scaler_y, ticker):
    model.eval()
    with torch.no_grad():
        predictions = model(X_val).cpu().numpy()
        actuals = y_val.cpu().numpy()

    # 1. ì—­ë³€í™˜ (0~1 -> ì‹¤ì œ ë‹¬ëŸ¬ ê°€ê²©)
    pred_price = scaler_y.inverse_transform(predictions)
    actual_price = scaler_y.inverse_transform(actuals)

    # 2. ì˜¤ì°¨ ê³„ì‚° (RMSE, MAE)
    mse = mean_squared_error(actual_price, pred_price)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(actual_price, pred_price)

    # 3. ë°©í–¥ ì •í™•ë„ (Directional Accuracy)
    # tì‹œì ì˜ ê°€ê²©ì´ t-1ì‹œì ë³´ë‹¤ ì˜¬ëëŠ”ì§€ ë‚´ë ¸ëŠ”ì§€ ë¶€í˜¸ê°€ ê°™ìœ¼ë©´ ì •ë‹µ
    # ì‹¤ì œ ë³€ë™í­: Actual[t] - Actual[t-1]
    # ì˜ˆì¸¡ ë³€ë™í­: Pred[t] - Actual[t-1] (ì£¼ì˜: ì˜ˆì¸¡ê°’ê³¼ 'ì´ì „ ì‹¤ì œê°’'ì„ ë¹„êµí•´ì•¼ í•¨)
    
    # ë°ì´í„°ê°€ ì‹œê³„ì—´ì´ë¯€ë¡œ ië²ˆì§¸ ë°ì´í„°ì˜ 'ì „ë‚  ê°€ê²©'ì€ i-1ë²ˆì§¸ ì‹¤ì œ ê°€ê²©ì„.
    # í•˜ì§€ë§Œ X_val ë°ì´í„°ì…‹ êµ¬ì„± ìƒ, y_val[i]ëŠ” ië²ˆì§¸ ì‹œí€€ìŠ¤ì˜ íƒ€ê²Ÿì„.
    # ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ ì „ì²´ ë°°ì—´ì—ì„œì˜ ì—°ì†ì„±ì„ ê°€ì •í•˜ê³  ê³„ì‚° (1ë²ˆì§¸ ì¸ë±ìŠ¤ë¶€í„° ë¹„êµ)
    
    actual_diff = actual_price[1:] - actual_price[:-1]
    pred_diff = pred_price[1:] - actual_price[:-1] 
    
    # ë¶€í˜¸ê°€ ê°™ìœ¼ë©´ True (ì˜¤ë¦„/ë‚´ë¦¼ ë§ì¶¤)
    correct_direction = np.sign(actual_diff) == np.sign(pred_diff)
    accuracy = np.mean(correct_direction) * 100

    print(f"\nğŸ“Š [{ticker}] ìµœì¢… ì„±ëŠ¥ í‰ê°€")
    print(f" - RMSE (í‰ê·  ì˜¤ì°¨): ${rmse:.4f}")
    print(f" - MAE  (ì ˆëŒ€ ì˜¤ì°¨): ${mae:.4f}")
    print(f" - Directional Accuracy (ë°©í–¥ ì •í™•ë„): {accuracy:.2f}%")

    # 4. ì‹œê°í™” (ìµœê·¼ 100ì‹œê°„)
    plt.figure(figsize=(12, 6))
    
    # ì „ì²´ ê¸°ê°„ ì¤‘ ë§ˆì§€ë§‰ 100ê°œë§Œ ì‹œê°í™”
    vis_len = 100
    plt.plot(actual_price[-vis_len:], label='Actual Price', color='blue', alpha=0.6)
    plt.plot(pred_price[-vis_len:], label='Predicted Price', color='red', linestyle='--')
    
    plt.title(f"{ticker} LSTM Prediction (Last {vis_len} Hours)")
    plt.xlabel("Time Steps")
    plt.ylabel("Price ($)")
    plt.legend()
    plt.grid(True)
    
    save_path = f"{RESULT_DIR}/{ticker}_prediction.png"
    plt.savefig(save_path)
    plt.close()
    print(f" - ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {save_path}\n")

if __name__ == "__main__":
    for ticker in TICKERS:
        train_lstm(ticker)